% !TEX root = ../thesis.tex
%
\chapter{Optimal control theory}\label{ch:oct}

\section{Introduction}\label{sec:ch2:intro}

In Chapter~\ref{ch:intro} we highlighted how the optimisation of plant disease management can be mathematically and computationally challenging. In this chapter we will introduce the mathematical field of optimal control theory (OCT), and how it can be used to optimise epidemiological controls. Poorly designed management strategies can lead to expensive failures of control, for example as happened with Dutch elm disease in the UK in the 1970s \citep{tomlinson_too_2010}, and with citrus canker in Florida in the 2000s \citep{gottwald_citrus_2007}. OCT provides a framework for finding time dependent, optimal strategies for simple models, which could be used to systematically generate potential management strategies for use in the real-world. Alongside expert-informed strategies this could enable a more mathematically robust system for making disease management policy decisions.

In this chapter we review the use of OCT in the epidemiology literature. We first describe the formulation of problem that OCT can solve, and give an overview of the mathematical background. We focus here on a review of applications of OCT in epidemiology and related fields, rather than the underlying mathematical theory. We then describe two main numerical methods that are commonly used to find solutions to the optimal control problem, before analysing the differences between them using a simple disease model in the next chapter.

\section{Background}\label{sec:ch2:background}

Optimal control theory (OCT) is the field of mathematics concerned with finding time-varying inputs to dynamic systems, optimised to maximise (or minimise) some performance metric. The dynamical system could be a system of ODEs, PDEs, or discrete difference equations, controlled by an input variable, or combination of variables, that can be varied from outside the system. The field developed as an extension to the calculus of variations, with roots also in classical control theory, random processes, and linear and nonlinear programming \citep{bryson_optimal_1996}. In the 1950s, Bellman developed dynamic programming, a method for finding optimal controls by recursively solving smaller problems \citep{bellman_dynamic_1957}. These methods struggle to solve realistic problems because of the `curse of dimensionality', where as the number of state and control variables increases, the memory required to solve the recursive problem becomes impractical.

Later work by \citet{pontryagin_mathematical_1962} extended work from the calculus of variations to find a necessary condition for the maximising path. This method explores extremal paths individually and so avoids dimensionality issues. This condition is contained in Pontryagin's famous `maximum principle', the main theory underlying much of OCT. We will now summarise this principle but we do not describe the mathematics here. Further details can be found in \citet{lenhart_optimal_2007} or \citet{hocking_optimal_1991}. The maximum principle relates to a differential equation system for a state vector $\vect{x}$, described as follows:
\begin{equation}
    \dot{\vect{x}}(t) = \vect{f}(\vect{x}(t), \vect{u}(t), t);\quad{}\vect{x}(0)=\vect{x}_0;\quad{}t\in{}\left[0, T\right]
\end{equation}
where $\vect{u}(t)$ is a time dependent control vector. The control function $\vect{u}(t)$ must be chosen so as to maximise the objective function $J$:
\begin{equation}
    J = \Psi{}(\vect{x}(T)) + \int_0^TL(\vect{x}(t), \vect{u}(t), t)\,\mathop{dt}
\end{equation}
where $\Psi$ is a salvage term, or payoff function, and $L$ is the Lagrangian of the problem. The overall optimisation problem is therefore:
\begin{subequations}\label{eqn:ch2:opt_problem}
    \begin{align}
        \max_{\vect{u}(t)} \quad{} &\null \Psi{}(\vect{x}(T)) + \int_0^TL(\vect{x}(t), \vect{u}(t), t)\,\mathop{dt} \label{eqn:ch2:opt_problem_a}\\
        \text{subject to} \quad{} &\null \dot{\vect{x}}(t) = \vect{f}(\vect{x}(t), \vect{u}(t), t) \label{eqn:ch2:opt_problem_b}\\
        &\null \vect{x}(0)=\vect{x}_0 \label{eqn:ch2:opt_problem_c}
    \end{align}
\end{subequations}

The maximum principle states that if $\vect{u}^*(t)$ and $\vect{x}^*(t)$ are optimal for this system, then there must exist an adjoint system $\vect{\lambda}(t)$ such that:
\begin{equation}
    H(t, \vect{x}^*(t), \vect{u}(t), \vect{\lambda}(t)) \leq H(t, \vect{x}^*(t), \vect{u}^*(t), \vect{\lambda}(t))
\end{equation}
for all time $t \in \left[0, T\right]$. The Hamiltonian $H$ is given by:
\begin{equation}\label{eqn:ch2:hamiltonian_general}
    H(t, \vect{x}(t), \vect{u}(t), \vect{\lambda}(t)) = L(\vect{x}(t), \vect{u}(t), t) + \vect{\lambda}(t)\cdot\vect{f}(\vect{x}(t), \vect{u}(t), t)
\end{equation}
and the adjoint system satisfies:
\begin{subequations}\label{eqn:ch2:adjoint_system}
    \begin{align}
        \dot{\vect{\lambda}}(t) &= \frac{\partial H(t, \vect{x}(t), \vect{u}(t), \vect{\lambda}(t))}{\partial \vect{x}} \label{eqn:ch2:adjoint_system_a}\\
        \vect{\lambda}(T) &= \frac{\partial \Psi(\vect{x}(T))}{\partial \vect{x}}\;. \label{eqn:ch2:adjoint_system_b}
    \end{align}
\end{subequations}
The terminal constraint in the adjoint system is known as the transversality condition. This maximum principle can then be used to find solutions to the optimisation problem described above. We next look at how this framework has been used in epidemiology.

\section{OCT within epidemiology}\label{sec:ch2:oct_in_epidem}

When deciding how to manage human, animal or plant epidemics, public health or environmental officials must take into account economic constraints. OCT optimises control strategies whilst ensuring adherence to constraints and minimisation of total costs, and is therefore a useful tool for designing epidemic interventions. Successful disease control relies on applying the most effective control methods at the correct time, and at a sufficient level. OCT is an effective tool for balancing disease management objectives with economic and logistic constraints, provided that the overall goal of the management program is well defined. OCT has been applied to models of many diseases, including HIV \citep{kirschner_optimal_1997}, mosaic virus in \emph{Jatropha curcas} plants \citep{al_basir_impact_2017}, and sudden oak death \citep{mbah_optimization_2010}. In this section we will review how OCT has developed within the field of epidemiology, discussing outstanding limitations and problems.

\subsubsection{Early work \& general principles}

Early work using OCT on epidemiological models was carried out by \citet{sethi_optimal_1978}, who optimised levels of vaccination and treatment in simple SEIR type models. The work is analytic and finds cases where the control can be classified as `bang-bang'. Bang-bang controls occur when the Hamiltonian (Equation~\ref{eqn:ch2:hamiltonian_general}) is linear in the control functions. The optimal control then switches between its maximum and minimum values giving so-called bang-bang strategies, where each control is either on or off. In bang-bang strategies there are often important switching times, when some or all of the controls switch from on to off or vice versa. These strategies have arisen repeatedly in disease control, for example by \citet{forster_optimizing_2007} for a plant disease spreading through an agricultural landscape, and \citet{panetta_optimal_2003} for cancer drug treatments.

As OCT was increasingly used within epidemiology, the underlying models increased in complexity. \citet{behncke_optimal_2000} developed models with more complex forms of control, including vaccination, quarantine and screening, and health-promotion campaigns. The interventions modelled are more realistic than those used in previous studies, but the models are analytic and abstract in nature. The models are not specific to the form of the force of infection, so that the results are valid in the general case. The control strategies found here introduce the concept of `front-loading', where additional resources are allocated early in the epidemic when control can be more effective. The importance of this early timing of control for successful management has also been shown for control of sudden oak death \citep{cunniffe_modelling_2016}.

\subsubsection{Bioeconomics}

Throughout its history, OCT has been closely connected with the field of economics \citep{weber_optimal_2011}, for example in fisheries economics \citep{clark_economics_1975}. Consideration of economics is important for optimising epidemiological controls \citep{perrings_merging_2014}, since interventions must be cost-effective, and resources are often limited. In one study, OCT was used to find cost-effective human papillomavirus vaccination strategies \citep{brown_role_2011}, showing how time-dependent strategies can be found that balance the costs of administering vaccines with the costs of treating infected individuals. In plant epidemiology, a recent study shows how control strategies for vector transmitted diseases can balance multiple economic costs \citep{bokil_optimal_2019}, including control costs, yield loss and long term costs from insecticide use. Work on sudden oak death has shown how OCT can find an optimal balance when allocating limited resources between surveillance and eradication measures \citep{ndeffo_mbah_balancing_2010}.

Bioeconomic studies such as the ones considered in the previous paragraph, rely on a clearly defined objective of control, with all costs quantified in the same units. By setting the relative contributions for each cost term, for example surveillance, vaccination and treatment costs, the objective function chosen defines what is to be considered cost-effective. \citet{epanchin-niell_economics_2017} argues that in bioinvasion management a key gap in current understanding is how to value environmental benefits, such as ecosystem services and biodiversity, alongside the costs and effectiveness of controls. The same difficulties are present in the epidemiological literature. Choosing an objective function that balances costs meaningfully is important though, since the choice of the objective function can make significant changes to the optimal strategy \citep{probert_decision_2016}.

\subsubsection{State of the art and outstanding questions}

As the use of OCT in epidemiology progressed, the complexity of the underlying control models increased to capture more realism and ask more applied questions. As examples, OCT has been used in models of vector-borne diseases applied to malaria \citep{blayneh_optimal_2009}, vaccination rates against \emph{Clostridium difficile} in a hospital setting \citep{stephenson_optimal_2017}, and, as previously mentioned, balancing of detection and eradication controls for sudden oak death \citep{ndeffo_mbah_balancing_2010}. Despite the additional complexity and realism, the underlying models in these and other studies are still relatively simplistic, and cannot be expected to capture complex realistic dynamics. OCT results have therefore rarely been tested in the field.

Occasionally, though, OCT results have been tested using more complex models to relax the assumptions in the simplistic OCT models. \citet{forster_optimizing_2007} find that in a mean field epidemic model a switching strategy is optimal. \citeauthor{forster_optimizing_2007} also test their mean field results on a spatial contact process model, and find that if the switch time is not known precisely then the OCT strategy can be worse than a simple constant strategy. Another study finds optimal control strategies for chlamydia, and tests the non-spatial results on a spatial network simulation \citep{clarke_approximating_2013}. The strategies found using OCT result in rapidly switching controls that perform much worse in the simulation model than in the OCT model, and so, for effective control in the real world, the OCT model should be constrained to avoid these rapid switches. Whilst a few examples do test OCT results on more realistic systems, this is by far not the norm, and understanding how epidemiological OCT results can be mapped to the real world is a significant gap in the literature.

Within plant epidemiology, the effect of space is highly important in both modelling the spread of disease, and designing effective management strategies \citep{ostfeld_spatial_2005, plantegenest_landscape_2007}. Some OCT studies have used simple metapopulation models to capture some element of spatial dynamics. \citet{ndeffo_mbah_optimal_2014} optimise control on a one-dimensional lattice, showing that optimal control tracks the wavefront of a spreading epidemic. An alternative approach to finding optimal spatial strategies is to use partial differential equation (PDE) models, as used by \citet{neilan_optimal_2011} for a model of rabies in raccoons. Other studies resort to simple epidemiological models and optimisations, for example \citet{epanchin_optimal_2012} use a nearest neighbour spread model. The spread model is formulated as an inequality system, allowing the control optimisation to be carried out using integer programming. Spatial control strategies can be difficult to optimise, but use of simpler models could result in strategies that are not robust when additional realism is included. Without explicit testing of the OCT strategies on a realistic spatial model, how can a policy maker be sure the strategies are robust?

\section{Optimisation methods}\label{sec:ch2:optim_methods}

There are many numerical methods for solving the optimisation problem described in Equations~\ref{eqn:ch2:opt_problem} (p.~\pageref{eqn:ch2:opt_problem}). These methods can be grouped into two main classes: indirect and direct methods. Indirect methods find roots of the necessary condition given by the Pontryagin maximum principle, whereas direct methods find a sequence of controls that minimise the objective function without using the adjoint system \citep{betts_practical_2010}. We will briefly describe a formulation using each of these methods, but refer to \citet{betts_practical_2010} for a comprehensive discussion of numerical methods for optimal control problems.

\subsection{Indirect formulation}

Indirect methods construct the necessary conditions given by the Pontryagin maximum principle for the system in question. This involves forming the Hamiltonian and the adjoint system. The necessary conditions are then used to find an expression for $\vect{u}^*(t)$ in terms of the optimal state ($\vect{x}^*(t)$) and the adjoint ($\vect{\lambda}(t)$). This can be used to solve for $\vect{x}^*(t)$ and $\vect{\lambda}(t)$ with the boundary conditions, finally allowing $\vect{u}^*(t)$ to be calculated. The process effectively optimises the system, then discretises to solve for the optimal control.

More specifically, since the initial conditions for the state system, and the terminal conditions for the adjoint system are known (Equations~\ref{eqn:ch2:opt_problem_c} and \ref{eqn:ch2:adjoint_system_b}, p.~\pageref{eqn:ch2:opt_problem_c}), then the necessary conditions become a two point boundary problem. This state-adjoint system will be solved when the control function is optimal ($\vect{u}^*(t)$). The forward-backward sweep method (FBSM) is a simple numerical algorithm for solving this formulation of the optimal control problem \citep{lenhart_optimal_2007}. The method is described in Algorithm~\ref{alg:ch2:fbsm}.

The FBSM is based in the necessary conditions for optimality. This means that the results are closely connected to the underlying mathematics of the optimal control theory. This can help to give more insight into the meaning of the resulting controls, since the underlying dynamics that influence the control are analytically described. There are however, a number of limitations to using the indirect formulation. Firstly, this close connection to the optimality conditions means that significant work is necessary to set up the optimisation problem. The Hamiltonian, adjoint dynamics, and transversality conditions must all be derived. This can be mathematically challenging, and is inherently inflexible since the equations must be derived for each new problem \citep{betts_practical_2010}. Further to this, there can be issues with the convergence of the FBSM. The convergence can be highly sensitive to the initial guess for the optimal control, and the method may never converge to a solution.

\begin{algorithm}[h]
    \caption{The forward-backward sweep method (FBSM) algorithm from \citet{lenhart_optimal_2007}, for solving optimal control problems using the indirect formulation.\label{alg:ch2:fbsm}}
    \begin{enumerate}
        \item Make an initial estimate for the control function, $\vect{u}(t)$.
        \item Using this control and the initial state condition $\vect{x}(0)=\vect{x}_0$, solve the state system forward in time.
        \item Using the adjoint terminal condition (Equation~\ref{eqn:ch2:adjoint_system_b}), and the values for control and state, solve the adjoint system backwards in time.
        \item Update the control $\vect{u}(t)$ using the new state and adjoint values, by maximising the Hamiltonian.
        \item Check for convergence. If the system has not converged to an optimal control, return to step 2 using the updated control.
    \end{enumerate}
\end{algorithm}

\subsection{Direct formulation}

The direct formulation differs from the indirect in that it does not rely on the derivation of the Hamiltonian or adjoint system. Direct formulation methods discretise the dynamic system first, and then optimise this, rather than optimising using the optimality conditions. One method for solving a direct formulation problem is the direct transcription process. This method discretises the state and control in time, and treats the values of the state and control at these discrete times as optimisation variables in a nonlinear programming (NLP) problem. Within the NLP problem, the state dynamics and initial conditions are included as constraints on the NLP variables, and the optimisation is carried out to minimise the objective value.

Whilst it may seem excessive to be directly optimising all the state variables, because each variable only directly influences variables that are close in time the problem becomes large but sparse. This sparsity can be exploited by numerical optimisation routines, and is often simpler to solve than a small, dense problem in which the state is not discretised and optimised \citep{betts_practical_2010}. The state dynamics constraints are constructed as a discretisation of the state ODE system, often using an implicit Runge-Kutta scheme. For example, following the standard approach in \citet{betts_practical_2010}, if the state-control system is discretised on $M$ grid points, the NLP variables are given by:
\begin{equation}
    \vect{y}^\mathrm{T} = \left(\vect{x}_1, \vect{u}_1, \ldots \vect{x}_M, \vect{u}_M\right)\;.
\end{equation}
Using an Euler discretisation scheme, the state ODEs ($\dot{\vect{x}} = \vect{f}(\vect{x}, \vect{u}, t)$) can be approximated by the following NLP constraints:
\begin{equation}
    \vect{0} = \vect{x}_{k+1} - \vect{x}_k - \frac{h_k}{2}\left(\vect{f}_{k+1} + \vect{f}_k\right) \equiv \vect{c}(\vect{y})
\end{equation}
The NLP variables $\vect{y}$ are then optimised subject to the constraints $\vect{c}(\vect{y})$ using an NLP solver.

The advantages of the direct over the indirect formulation are in its flexibility and robustness. Since the Hamiltonian and adjoint are not required, and gradients can be estimated using finite differences, the method can be used for any system without analytic derivatives. Furthermore, there is extensive literature and software available for solving large, sparse NLP problems. This means the optimisations can be fast, efficient and robust. Whilst the method does still require an initial guess for both the control and the state variables, the NLP problem has a much larger region of convergence than the root finding in indirect methods \citep{betts_practical_2010}.

\section{Conclusions}

In conclusion, OCT has allowed epidemiology to optimise control strategies whilst taking economic factors into account. Strategies often involve bang-bang solutions, where control is either maximal or minimal, leading to policies with precise switching times when priorities change. For finding these optimal strategies, there are two distinct classes of numerical method: the indirect method that is more closely connected to the underlying mathematics but that requires more analysis, and the direct method that is more flexible and robust but provides less insight.
