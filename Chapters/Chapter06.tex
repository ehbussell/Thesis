% !TEX root = ../thesis.tex
%
\chapter{Optimising stand level disease control to protect tanoak\label{ch:protect_tanoak_control}}

\section{Introduction\label{sec:ch6:intro}}

In the previous chapter we implemented a model of stand level SOD dynamics. We adapted the Cobb model \citep{cobb_ecosystem_2012} to incorporate realistic spatial dispersal properties, and reparameterised it to ensure the invasion timescales were still realistic. We now have a simulation model representative of SOD spread within a mixed species forest stand. In this chapter, we ask how control strategies can be optimised using this model, addressing the key question of how time-dependent controls should be deployed to best protect tanoak. As in the Chapter~\ref{ch:complex_models}, we will require an approximate model to optimise control since the simulation model is too complex. We will then use the open-loop and MPC frameworks we previously developed to integrate control strategies into the simulation model, and demonstrate the importance of continued surveillance for effective control.

Using our optimal control frameworks we will explore what control methods are most effective, and how deployment should vary over time. Most importantly, we will look at the effects of parameter and observational uncertainty on control efficacy, testing under what conditions MPC outperforms open-loop\footnote{All code for this chapter is available at \url{https://github.com/ehbussell/MixedStand}}.

\section{Stand level disease control\label{sec:ch6:control}}

In this section we describe the methodology for optimising control in the mixed stand model. We develop a non-spatial approximate model of the dynamics, incorporating thinning, roguing, and protectant controls. These are then optimised and applied using the open-loop and MPC frameworks. It is first necessary to quantify the effectiveness of a given strategy by defining the purpose of control in this system.

\newpage
\subsection{Management objectives}\label{sec:ch6:mgmt_objs}

As described in Section~\ref{sec:ch5:Intro}, it is important to retain tanoak in the forest stand due to its high cultural and ecological value. We therefore investigate strategies for specifically protecting tanoak, aiming to maintain a population of healthy overstorey tanoak trees in the stand over the medium- to long-term. Following the timescales in \citet{cobb_ecosystem_2012} we look at preventing decline of tanoak over a time horizon ($T$) of 100~years. Since the focus of the control is to ensure tanoak still exists in the future, we treat this as a terminal objective function. By this we mean that the goal is to maximise the number of healthy overstorey tanoak trees at the end of the time horizon---i.e.\ after 100~years---rather than integrated over that time period.

This objective does not capture everything that is important to maintaining a healthy forest though. A control strategy that protects overstorey tanoak to the detriment of all other trees is clearly suboptimal, and this should be accounted for in the objective function. Trees in forests are important for wildlife habitats and food sources, recreational uses and carbon fixation \citep{swiecki_reference_2013}, and maintaining diversity ensures that these and other important ecosystem services are provided \citep{cadotte_beyond_2011, gamfeldt_higher_2013}. Beyond this, diverse forests are more resilient to other disease threats \citep{keesing_impacts_2010}; there is little point to a control strategy that protects tanoak from SOD but makes the forest vulnerable to attack from another pathogen. The management goal must therefore capture a balance between protection of tanoak, and continued host diversity for provision of ecosystem services. The balance between these two objectives, however, will depend on the overall local management goals for the forest stand in question and economic valuation of the ecosystem services \citep{thompson_forest_2011}.

There are many possible measures of diversity that could be used as part of the management objective. One measure that is very popular in the ecological literature is the Shannon index \citep[][pp.\ 106--108]{magurran_measuring_2013}. The Shannon index originates in information theory, based on the idea that diversity is a measure of the expected information content when observing the species of a random individual. The Shannon index $H'$ is calculated using this equation:
\begin{equation}\label{eqn:ch6:shannon_idx}
    H' = -\sum_ip_i\ln{p_i}
\end{equation}
where $p_i$ is the proportion of individuals in species $i$. It measures both species richness and species evenness, so is suited to our application in which the number of different species and evenness across species are both important. The Shannon index can be transformed into a value with more biological meaning: the effective number of species (ENS). This measures how many equally common species would be required to achieve the same level of diversity, and for the Shannon index $H'$ is given by:
\begin{equation}\label{eqn:ch6:shannon_ens}
    \textrm{ENS} = \exp(H')\;.
\end{equation}

We seek a control scheme that maintains ecosystem services, so it is important that diversity is maintained throughout the simulation time. This ensures that further diversity is not lost in wildlife and other plant life during the epidemic. We therefore choose this to be an integrated metric, integrated over the full time horizon $T$. Overall then, our management objective is made up of a terminal term corresponding to preserving healthy large tanoak, and an integrated term to maintain maximum diversity across all times. The mathematical form of the objective is given by:
\begin{equation}\label{eqn:ch6:mgmt_obj}
    J = \gamma_1\left(S_{1,3}(T) + S_{1,4}(T)\right) - \gamma_2\left(\int_{t=0}^T \sum_ip_i\ln{p_i}\,\mathop{dt}\right)
\end{equation}
where $S_{1,3}$ and $S_{1,4}$ are the densities of healthy tanoak in the third and fourth age classes respectively, and $\gamma_1$ and $\gamma_2$ are the weights associated with the tanoak retention and diversity conservation objectives respectively. There is an arbitrary choice in the balance between these two terms, which must be chosen by a policy maker or forest manager. In our case the weights are set such that in the disease free case, the contribution of the tanoak retention and biodiversity terms to the overall objective function are equal to 1 and 0.25 respectively. Whilst this is an arbitrary choice, we scan over the relative diversity benefit ($\gamma_2$) later in Section~\ref{sec:ch6:div_scan}.

\subsection{Control methods}

Many different methods are recommended for controlling the spread of \textit{P.~ramorum} \citep{swiecki_reference_2013}. However, the methods can be grouped into three main classes: roguing, thinning and protecting. Roguing methods are based on finding and removing infected hosts, whereas thinning methods remove hosts regardless of infection status. Removal of hosts, either through thinning or roguing, is the only control that has been effective at the landscape scale \citep{hansen_epidemiology_2008}. Management recommendations made by the U.S.\ Forest Service highlight removal of the spreader species bay as very important for effective control \citep{swiecki_reference_2013}, but also recommend removal of infected hosts.

Whilst host removal is the only effective method at the landscape scale, protection methods could also be useful at the smaller scales of a single forest stand. These methods apply chemicals to uninfected trees to reduce their susceptibility to the disease. For SOD the main protectants used are phosphonates, that are approved for use on oak and tanoak species. The treatment only works as a preventative measure but it is recommended for protecting individual hosts \citep{lee_protecting_2010}. Reports of the effectiveness of phosphonate treatment vary, but most studies suggest it does slow infection \citep{swiecki_reference_2013}. Application by bark-spray or trunk injection is reported to be effective for up to around two years \citep{garbelotto_phosphonate_2009}, but evidence is lacking for the impact on host susceptibility. There are also conflicting reports about its efficacy, with some studies finding little effect of treatment \citep{kanaskie_application_2011}. Here we will assume a mild effect of \SI{25}{\percent} reduction in susceptibility.

Roguing controls can be applied separately to infected small tanoak, large tanoak and bay laurel. The hosts are removed and do not resprout, consistent with application of a herbicide to the stump as is often recommended \citep{swiecki_reference_2013}. Thinning removes hosts of all infection status, and can be applied separately to small tanoak, large tanoak, bay and redwood. Protection can only be applied to small and large tanoaks, and only to susceptible hosts. These hosts are moved into new protected class with the same demographic dynamics (i.e.\ there is a protected class $P_{1,i}$ for each age class $i$ of tanoak). The protected classes have reduced susceptibility (by \SI{25}{\percent}) but return to the susceptible class at a rate of \SI{0.5}{\per\year}. This corresponds to an average time of \SI{2}{\years} before protection wanes. Table~\ref{tab:ch6:control_methods} summarises all the control methods and their effects.

\begin{table}[h]
    \centering
    \caption[Possible control methods implemented in the stand level model]{Possible control methods implemented in the stand level model. There are three main groups of control: roguing, thinning and protecting, and these can be targeted at different host groups. Approximate costs of each control are taken from \citet{kovacs_predicting_2011}, but roguing costs are increased to account for additional costs of identification and removal of unstable diseased trees.\label{tab:ch6:control_methods}}
    \begin{tabular}{@{}llcc@{}}
        \toprule
        \textbf{Control} & \textbf{State changes} & \textbf{Rate} $\eta_I$ / \si{\per\year} & \textbf{Cost} $c_i$ / a.u.\\
        \midrule
        Rogue small tanoak & $I_{1, 1\text{--}2} \rightarrow \emptyset$ & 0.25 & 3000\\
        Rogue large tanoak & $I_{1, 3\text{--}4} \rightarrow \emptyset$ & 0.25 & 6000\\
        Rogue bay & $I_{2} \rightarrow \emptyset$ & 0.25 & 6000\\
        \midrule
        Thin small tanoak & $\{S, I, P\}_{1, 1\text{--}2} \rightarrow \emptyset$ & 1.0 & 250\\
        Thin large tanoak & $\{S, I, P\}_{1, 3\text{--}4} \rightarrow \emptyset$ & 1.0 & 500\\
        Thin bay & $\{S, I\}_{2} \rightarrow \emptyset$ & 1.0 & 500\\
        Thin redwood & $S_{3} \rightarrow \emptyset$ & 1.0 & 500\\
        \midrule
        Protect small tanoak & $S_{1, 1\text{--}2} \rightarrow P_{1, 1\text{--}2}$ & 0.25 & 200\\
        Protect large tanoak & $S_{1, 3\text{--}4} \rightarrow P_{1, 3\text{--}4}$ & 0.25 & 200\\
        \bottomrule
    \end{tabular}
    \end{table}

\subsubsection{Budget constraint}

For each of the 9 controls in Table~\ref{tab:ch6:control_methods}, we seek a time-varying control parameter $f_i(t)$ between zero and one, indicating the level of control $i$ that minimises the management objective function. To model economic and logistic constraints we limit the total expenditure per unit time, where this is the product of the number of hosts controlled and the cost of that control method. The mathematical form of this constraint is given by:
\begin{equation}
    \sum_i \left(f_i\eta_iX_i\right)c_i \leq B
\end{equation}
where $X_i$ is the stem density of the controlled hosts. For example, for roguing of small tanoak $X_i$ would be $\left(I_{1,1} + I_{1,2}\right)$. The term in brackets is therefore the rate of removal of hosts for each control. The cost of each control is given by $c_i$ and the maximum budget is given by $B$. Whilst the costs are chosen somewhat arbitrarily because of a lack of data, the scales are informed by the results of \citet{kovacs_predicting_2011}. We include higher costs for roguing to capture the additional costs with identification and removal of unstable infected trees.

\subsection{Approximate model}

Optimisation of the chosen management objective using all nine time-dependent controls is computationally infeasible using the full spatial model. To allow progress we use an approximate model and lift control results from this simpler optimisation back to the simulation model, using the methods described in Chapter~\ref{ch:complex_models}. Here we choose to make the approximate model non-spatial, so as to significantly reduce the state-space for optimisation. Further approximations could be possible, for example grouping together the age classes within the small and large tanoak groups. To ensure that we can lift demographic parameters directly from the simulation model however, we only change the spatial structure of the model. The approximate model therefore assumes that all hosts in the forest stand are well-mixed.

As all other features of the model are retained, the form of the equations is very similar to that of the simulation model (Equations~\ref{eqn:ch5:cobb_model_1}) so we will not repeat them here. The only difference is that in the approximate model the dependence on cell ($x$) of the states and empty space ($E$) has been dropped. The form of the force of infection terms is simpler since infection no longer spreads between cells. The infection rates in the non-spatial approximate model cannot be lifted from the simulation model, since the approximate model now assumes infection comes from all infected hosts in the stand rather than just those in the immediate spatial vicinity. The force of infection terms in the approximate model are therefore:
\begin{subequations}\label{eqn:ch6:infection_approx}
        \begin{align}
            \tilde{\Lambda}_{1,i} &= \tilde{\beta}_{1,i}\sum_{j=1}^4I_{1,j} + \tilde{\beta}_{12}I_{2} \\
            \tilde{\Lambda}_{2} &= \tilde{\beta}_{21}\sum_{j=1}^4I_{1,j} + \tilde{\beta}_{2}I_{2}
        \end{align}
\end{subequations}
where $\tilde{\beta}$ indicate infection parameters that need to be fitted to the simulation model.

\subsubsection{Fitting infection rates}

The seven infection rates $\tilde{\beta}$ are the only parameters that must be fitted to the simulation model. The approximate model cannot capture the heterogeneous mixing present in the simulation model, however, the approximated dynamics may be accurate enough to give effective control strategies when optimised. We use the method of least squares to match the simulation and approximate models since both models are deterministic. To fit the parameters, the simulation model is used to run a single trajectory with no control interventions. For the fitting process we use the same initial conditions as described in the previous chapter, with infection seeded in the centre of a 20 by 20 grid of cells. The disease progress curves of this simulation realisation are then used as the baseline for fitting the approximate model. For a trial set of $\tilde{\beta}$ parameters and an approximate model trajectory, we calculate the sum of squares as the sum of squared deviations between the simulation and approximate disease progress curves for each age class of tanoak, and for bay, at time points throughout the trajectory. The $\tilde{\beta}$ parameters are then optimised by minimising this total summed squared error (SSE). For a set of time points $t_i$, and where approximate model states are signified with a tilde, the equation for SSE is given by:
\begin{equation}
    \mathrm{SSE} = \sum_{i}\left[\sum_{j=1}^4\left(I_{1,j}(t_i) - \tilde{I}_{1,j}(t_i)\right)^2 + \left(I_{2}(t_i) - \tilde{I}_{2}(t_i)\right)^2\right]
\end{equation}
where the dependence on cell in the simulation terms has been dropped to indicate an average over all cells in the landscape, for example:
\begin{equation}
    I_{1,j}(t) = \frac{\sum_xI_{1,j,x}(t)}{N_\textrm{cells}}\;.
\end{equation}
An average is used so that the approximate model tracks stem density in the same units as the simulation model.

In the simulation model, infectious pressure is dominated by sporulation from bay laurel. This makes estimation of all `within-tanoak' infection rates ($\beta_{1,i}$) difficult, as from the simulation data they are individually unidentifiable. We therefore use a two stage fitting process. For the first stage, all infection rates in the simulation model related to bay ($\beta_2$, $\beta_{12}$, and $\beta_{21}$) are set to zero. This makes bay epidemiologically inactive, but maintains the same demographic dynamics. The simulation model is run using these parameters, and the SSE is minimised to find the within tanoak-infection rates.

In the second fitting stage all infection rates are fitted, with bay epidemiologically active again. The within-tanoak rates relative to $\beta_{1,1}$ from the first stage are used as a constraint to ensure the identifiability of these rates. This means a single within-tanoak rate is fitted in stage 2, with all other within-tanoak rates fixed relative to this using the results from stage 1. This stage also fits the bay infection rate, and the cross-species infection rates.

As can be seen in Figure~\ref{fig:ch6:approx_fit}, despite lacking any spatial component, the approximate model can very closely capture the uncontrolled dynamics of the simulation model. However, the approximate model should also fit as accurately as possible when control strategies are introduced. In Figure~\ref{fig:ch6:fit_under_control}, the fit of the approximate model is tested under constant control strategies using fixed control rates. It is clear that roguing at the same rate is more effective in the approximate model. This is because of the difference in mixing between the approximate and simulation models. The effect is small for thinning and protecting, but the same level of roguing in the approximate and simulation models gives very different dynamics.

\begin{figure}[t]
    \begin{center}
        \includegraphics{Graphics/ch6/Approx_fit}
        \caption[Fitting of approximate model]{Fitting of the approximate model to match the output of the simulation model. \textbf{(a)} shows the overall stem density for each species class, with the dashed line showing the fitted approximate model. The fit is carried out by matching the disease progress curves as shown in \textbf{(b)}\label{fig:ch6:approx_fit}.}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Control_fit_test}}
        \caption[Testing of approximate model under control]{Testing of approximate model under constant control strategies. The rows from top to bottom show dynamics under constant roguing, thinning, and protecting strategies. The left plots show overall host dynamics, and the right plots show the infected host dynamics. The roguing and protecting strategies control at the maximum rates from Table~\ref{tab:ch6:control_methods}, whereas the thinning strategy controls at \SI{10}{\percent} of the maximum rate. The approximate model fits well under constant thinning and protecting strategies, but less well under a constant roguing strategy.\label{fig:ch6:fit_under_control}}
    \end{center}
\end{figure}

\subsubsection{Empirical parameterisation of control}\label{sec:ch6:control_scaling}

Roguing is less effective in the simulation model because of an imposed spatial structure in the non-spatial control strategy. Roguing in the simulation model removes infected hosts from the core and edge of the spreading epidemic. Removal of hosts from the core has little effect on the rate of epidemic spread, since they are not near the wavefront. In the non-spatial model however, all hosts are well-mixed, so removal of infected hosts has a larger effect.

As a simple correction for this difference in roguing effectiveness, we investigate a simple scaling of the roguing rate in the approximate model. To test the plausibility of a single scaling rate for all approximate roguing controls, both models are run with constant roguing strategies. Roguing of small tanoak, large tanoak and bay are all set to occur at the same rate for the whole simulation, and this rate is varied between simulations. In the approximate model, the control rate is scaled by a single parameter which is also varied, and we analyse the difference in the final number of healthy large tanoak after \SI{100}{\years} between the simulation and approximation (Figure~\ref{fig:ch6:control_scaling}). To minimise the deviation across a range of control rates, the value of the scaling parameter is optimised. We optimise the deviation in the number of large healthy tanoak, since this is the primary objective of the control and must therefore be captured as accurately as possible. The optimisation minimised the sum of squared errors (SSE) over the range of control rates.

The results in Figure~\ref{fig:ch6:control_scaling} show that a single scaling factor can largely eliminate the deviation under constant roguing strategies. We do not expect this scaling to ensure that the approximate model is always closely aligned to the simulation model, particularly once control strategies are time-varying. The approximate model simply cannot capture the heterogeneities in host mixing. This scaling does however go some way to ensuring that control strategies from the approximate model perform well on the simulation model.

\begin{figure}[t!]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Control_scaling}}
        \caption[Scaling of control rates]{Empirical scaling of approximate model control rates to match simulation output. \textbf{(a)} shows the difference in the final number of large healthy tanoak as a function of the constant roguing rate, for a number of control rate scaling factors. The optimal value minimises the sum of squared errors (SSE) over all rates, as shown in \textbf{(b)}, and is found to be \num{0.682} (3~s.f.). \textbf{(c)} and \textbf{(d)} show the dynamics with the newly scaled approximate control rates, under a constant roguing strategy. The approximate model now fits the overall host dynamics well, but to do this slightly overestimates the level of infection.\label{fig:ch6:control_scaling}}
    \end{center}
\end{figure}

\subsection{Control frameworks and lifting}\label{sec:ch6:lifting}

In Chapter~\ref{ch:complex_models} we introduced the open-loop and MPC control frameworks. These same frameworks are tested here. To aid convergence, the controls inputs $f_i$ are constrained in the optimisation to be held constant over \SI{5}{\year} stages. The main difference from Chapter~\ref{ch:complex_models} in applying the control frameworks is how controls are lifted from the approximate model to the simulation model. The budget constraint is a mixed constraint that couples the control inputs ($f_i$) with the state of the system ($X_i$). When the control inputs are lifted to the simulation model, there is no guarantee of the states being exactly the same, and so the expenditure by the control will not be the same. This can mean that direct lifting of the control inputs will lead to the budget being exceeded. When the budget is exceeded, the control inputs are multiplied by a factor to reduce the overall expenditure to meet the budget constraint. This avoids imposing a priority amongst control methods which would have to be chosen arbitrarily. In mathematical terms, the corrected control inputs ($f_i'$) from the approximate model depend on the state in the simulation ($X_i$) in the following way:
\begin{equation}
    f_i' = f_i\frac{B}{\sum_j\left(f_j\eta_jX_jc_j\right)}\;.
\end{equation}
This is only the case when the budget is exceeded, otherwise the control inputs $f_i$ are used directly. We do not correct for under-allocation of resources, i.e.\ control inputs not spending the entire budget, since this could lead to extra control not accounted for in the approximate model. In practice correcting for this would lead to extra resources allocated to thinning, and hence removal of more healthy trees than is necessary.

\section{Results\label{sec:ch6:results}}

\subsection{Optimal strategies}

We first show the optimal strategies found using OCT on the approximate model. The strategies are lifted to the simulation model using the methods described above for both open-loop and MPC frameworks. Note that here control starts \SI{6}{\years} after the initial conditions used for fitting. This is to allow time for the disease to become established, making detection of the epidemic more likely. The initial conditions are therefore found by running the simulation model with no control for \SI{6}{\years}.

\subsubsection{Open-loop strategies}\label{sec:ch6:open_loop}

The open-loop framework carries out control optimisation on the approximate model, and this strategy is then lifted to the simulation for the full duration of the epidemic. The strategy found using OCT when applied to the simulation is shown in Figure~\ref{fig:ch6:strats}(a). The strategy focusses on thinning of bay, followed by thinning of redwood, early in the epidemic. Roguing is carried out throughout the epidemic but at a rate which increases towards the end. This is because late in the simulation infection re-emerges, so roguing uses more of the budget than anticipated by the approximate model. There are more infected hosts in the simulation, so it costs more to remove them. This can also be seen in Figure~\ref{fig:ch6:strats}(b), where towards the end of the epidemic in the simulation, there is a decline in the tanoak numbers. This is not captured by the approximate model that anticipates tanoak numbers continuing to increase.

\begin{figure}
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Control_results}}
        \caption[Open-loop and MPC optimal control strategies]{Optimal allocation of control resources using the approximate model. \textbf{(a)} shows the allocation over time to each control method using the open-loop framework. Control proportions ($f_i$) are fixed over 5 year intervals, but as the number of hosts changes during each interval, this leads to variable expenditure over the 5 years. Greyed out control methods in the legend are not used in either strategy. Heavy thinning of bay and then redwood is carried out early in the epidemic, with protectant controls only used once resource intensive thinning has been completed. In \textbf{(b)} the corresponding host dynamics are shown for both the simulation and approximate models. The approximation degrades towards the end of the epidemic, leading to unanticipated tanoak decline in the simulation. The blue bar highlights the difference between the approximate and simulation models in the density of large tanoak. \textbf{(c)} shows the MPC resource allocation, updated every 20 years, with a similar pattern to the open-loop strategy. However, additional thinning is carried out after each update later in the epidemic to manage the bay population. The corresponding host dynamics in \textbf{(d)} show that MPC repeatedly resets the approximate model trajectory, allowing more informed control decisions. This minimises the tanoak decline seen using open-loop, and gives a much lower error in the estimate of large tanoak stem density (blue bar).\label{fig:ch6:strats}}
    \end{center}
\end{figure}

The open-loop strategy carries out a large amount of thinning early in the epidemic. As can be seen in Figure~\ref{fig:ch6:strat_compare}(a), this severely impacts the stand diversity. Over the course of the epidemic though, the diversity in the simulation returns to close to its initial value. In the approximate model diversity is not expected to recover as much. This reduction in diversity is balanced though, by the retention of tanoak in the stand. As shown in Figure~\ref{fig:ch6:strat_compare}(b), the approximate model expects the control strategy to restore the full tanoak population, and increase it above its initial value. It is able to do this in the approximate model as disease is eliminated early, and healthy tanoak can grow into the space made available by the thinning of bay and redwood. This does not happen in the simulation model though, where the late re-emergence results in rapid decline of large healthy tanoak over the final \SI{20}{\years}. Despite this, the open-loop strategy shows a significant improvement over the dynamics under no control intervention. The strategy slows the spread of disease, keeping tanoak in the forest population for an additional \SI{80}{\years}.

\begin{figure}[t]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Control_comparison}}
        \caption[Comparing the open-loop and MPC frameworks]{Comparing performance of the open-loop and MPC frameworks. \textbf{(a)} shows the effect of control on the biodiversity in the forest stand, presented as the effective number of equally-common species. Any control---be it optimised via open-loop or MPC---is damaging to diversity, but MPC in fact has a slightly larger impact. The simulation and approximate model dynamics are shown, with the approximate model resetting every 20 years under MPC. \textbf{(b)} shows the stem density of healthy large tanoak over time, with significant decline under no control. The open-loop strategy slows tanoak decline, but MPC is more effective. In \textbf{(c)} the overall performance of the strategies as measured by the objective function is shown. MPC is slightly more damaging to diversity than open-loop, but this is balanced by retaining significantly more healthy tanoak.\label{fig:ch6:strat_compare}}
    \end{center}
\end{figure}

\subsubsection{MPC strategies}

In the MPC framework the approximate and simulation models are run concurrently, with the approximate model reset to match the simulation and re-optimised at regular update steps. These updated controls are then lifted to the simulation model going forward until the next update time. We first test the MPC framework with updates every \SI{20}{\years}. For the first \SI{20}{\years} the control is exactly the same as the open-loop strategy (Figure~\ref{fig:ch6:strats}(c)), since it is lifted from the same optimisation. After this, though, updates result in increased levels of thinning. In particular, after the updates at 60 and \SI{80}{\years} there is significant thinning of bay. This is to stem increased infection in bay, and keep the late disease re-emergence under control. As can be seen in Figure~\ref{fig:ch6:strats}(d), the updates ensure the approximate model dynamics more closely match the simulation, compared to under open-loop control. The extra thinning of bay slows the disease re-emergence and there is less decline in tanoak numbers.

Figure~\ref{fig:ch6:strat_compare}(a) and (b) show the diversity and tanoak retention under MPC\@. MPC is more damaging to diversity than open-loop. Although the approximate model incorrectly still expects tanoak numbers to increase after each update, keeping the model close to the simulation dynamics does improve control. The MPC framework retains approximately \SI{60}{\percent} of the original tanoak population after \SI{100}{\years}. Open-loop only retains \SI{15}{\percent}. Figure~\ref{fig:ch6:strat_compare}(c) compares the objective function values for open-loop, MPC, no control, and without any disease. We can see that MPC lowers diversity performance slightly compared to open-loop, but in doing this retains significantly more tanoak.

Since the updates in MPC improve control, an important question is how often to update the approximate model. Figure~\ref{fig:ch6:mpc_update}(a) shows the effect of changing the update period on the objective. We can see that as updates are made more frequent, control performance generally improves. This is because the approximate model can more closely match the simulation and hence appropriate control decisions can be made. There is, however, a dip in performance at update periods of around \SI{50}{\years}. This is due to the precise timing of the updates. The late disease re-emergence occurs after around \SI{80}{\years}. Update periods of around \SI{50}{\years} will not update close to this outbreak, and so the MPC framework cannot respond to that unexpected increase in infection. This results in ineffective control.

\begin{figure}
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/MPC_update}}
        \caption[Effect of MPC update period]{Effect of the MPC update period on control performance. \textbf{(a)} shows the objective value as a function of how often the MPC re-optimises control. Reducing the time until the next update generally improves control, although update periods of around \SI{50}{\years} perform worse than expected. This is because these periods do not update close to the late outbreak, and therefore miss the unexpected increase in infection. \textbf{(b)} and \textbf{(c)} show the control allocations for update periods of 5 and 100 years respectively. The low frequency control here corresponds to open-loop control. The high frequency control results in more continued thinning than in the low frequency control.\label{fig:ch6:mpc_update}}
    \end{center}
\end{figure}

Figures~\ref{fig:ch6:mpc_update}(b) and (c) show the MPC control for update periods of 5 and 100 years respectively. The low frequency update corresponds to open-loop control. We can see that the main difference as update frequency increases, is additional continued thinning of bay. This results in less roguing being required later in the epidemic.

\FloatBarrier
\subsection{Robust control}\label{sec:ch6:robust}

In this section we test the robustness of the results so far. In particular we analyse how parameter choices affect the control strategies selected by the open-loop and MPC frameworks, and their performance. We then investigate how the open-loop and MPC frameworks handle uncertainty in parameters and imperfect state estimation, i.e.\ not carrying out perfect sampling of the forest stand at the MPC update steps. These robustness analyses are important since the data available for fitting within-stand dynamics is limited.

\subsubsection{Budget sensitivity}

First we analyse the effect of the budget constraint. The maximum expenditure was chosen arbitrarily, so how much effect does it have on the optimal control strategy? Figure~\ref{fig:ch6:budget_scan} shows that in general the control strategy does not depend strongly on the budget. Across all budgets apart from the very smallest, largely the same set of control methods is used. As the budget increases, open-loop can allocate more resources to thinning, and with improved control both frameworks increase resources allocated to protection. It can also be seen that performance generally increases with increasing budget as might be expected. Kinks in this trend are due to some levels of control leading to a closer fit between the simulation and approximation, and hence improved control. At very high budgets control performance starts to degrade with budget. This is because in these cases the control is very effective in the approximate model, and so less control is carried out resulting in worse control in the simulation. As explained on page~\pageref{sec:ch6:lifting}, under-allocation of resources is not corrected for, although in this case the problem arises because a higher proportion of the available resources is allocated to protective controls. This results in less resource allocated to roguing, and hence less effective control.

Interestingly, only at very low budgets---where neither control framework is very effective---do we see thinning of small tanoak. In these cases there are not enough resources to control the disease effectively with roguing of tanoak alone. The next best option in this case is thinning of small tanoak, which reduces infection but also reduces numbers of healthy tanoak. This is therefore only chosen as a control method when roguing alone is not enough, and only in small amounts.

\begin{figure}[H]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Budget_scan}}
        \caption[Varying the control budget]{Effect of resource budget on control. \textbf{(a)} shows overall allocation of resources to each control method as a function of the maximum budget. Left hand bars show the results for open-loop, right hand bars for MPC\@. We can see that MPC generally allocates more to thinning, and requires less roguing. \textbf{(b)} shows the corresponding simulation objectives for the open-loop and MPC frameworks. Control generally improves as the budget increases. Interestingly, only at the lowest budget is any resource allocated to thinning of small tanoak, as shown in \textbf{(c)}.\label{fig:ch6:budget_scan}}
    \end{center}
\end{figure}

\subsubsection{Diversity benefit sensitivity}\label{sec:ch6:div_scan}

Next we test sensitivity to the relative benefit of diversity compared to tanoak retention. In the base case the diversity benefit is chosen such that in the disease free case, the diversity term will be \SI{25}{\percent} of the tanoak retention term in the objective function (as described in Section~\ref{sec:ch6:mgmt_objs}). In Figure~\ref{fig:ch6:div_scan} the control allocations and performance are shown, scanning over relative diversity benefit from 0 to \SI{100}{\percent}. It can be seen that, unsurprisingly, the best protection of tanoak, using both open-loop and MPC frameworks, is possible when there is no diversity benefit. As the diversity benefit increases, open-loop allocates fewer resources to thinning, and performance degrades. MPC on the other hand, is able to adapt and maintains high levels of tanoak protection through to the highest diversity benefits.

\begin{figure}[t]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/DivCostScan}}
        \caption[Varying the diversity benefits]{Changes in control strategy and performance as the relative benefit of diversity is changed. \textbf{(a)} shows overall allocation of resources to each control method as the diversity benefit is varied. \textbf{(b)} shows the stem density of large healthy tanoak retained in the stand. Low diversity benefits result in more thinning, but improved retention of tanoak in the open-loop case. MPC is able to respond to changes in diversity and retain tanoak for all values of the diversity benefit.\label{fig:ch6:div_scan}}
    \end{center}
\end{figure}

In Figure~\ref{fig:ch6:div_compare} we compare the control strategies and host dynamics with no diversity benefit, and the highest level of benefit. We can see that when there is no benefit to diversity protection, high levels of thinning are carried out that remove all bay and redwood trees. This leads to very effective disease control but for many forest managers and conservationists this would be an unacceptable cost to slow decline of a single important species \cite{noss_redwood_2000}. This demonstrates why diversity should be accounted for in the management objective. At the highest diversity benefit all species are retained in the system, but under open-loop control the disease is not controlled well towards the end of the epidemic. The same patterns are seen with the MPC framework, but the updates allow tanoak retention whilst also preserving diversity.

\begin{figure}[H]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Div_cost_compare}}
        \caption[Host dynamics when varying the diversity benefits]{Open-loop control and host dynamics are shown for low diversity benefit (\textbf{(a)} and \textbf{(b)}) and high diversity benefit (\textbf{(c)} and \textbf{(d)}). Additional thinning is carried out when diversity benefits are low, resulting in complete removal of all bay and redwood trees. Similar strategies are seen using the MPC framework.\label{fig:ch6:div_compare}}
    \end{center}
\end{figure}

\subsubsection{Parameter sensitivity}

We next analyse the sensitivity of the control strategies to the underlying model parameterisation. As in Section~\ref{sec:ch5:model_sensitivity}, all parameters from Table~\ref{tab:ch5:parameters} were randomly perturbed using a truncated normally distributed error with standard deviation of \SI{25}{\percent}. For each perturbed parameter set the approximate model was re-fitted, and control optimised using both the open-loop and MPC frameworks. The control strategies are compared across parameter sets by visualising the proportion of the budget that is allocated to each control class (thinning, roguing and protecting) over time. By sorting the parameter sets according to the objective function value, we test for any systematic differences in control strategy, for example larger epidemics requiring more thinning.

Figure~\ref{fig:ch6:ol_sensitivity} shows the ordered control strategies using the open-loop framework. There is a very clear shared structure to all control strategies, with thinning always carried out early in the epidemic. Roguing is used throughout, and protection is only used once resource-intensive thinning has lowered bay and redwood densities enough for disease suppression and tanoak promotion in the approximate model. There is no strong systematic pattern to the strategies once ordered by objective value.

\begin{figure}[t]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Sensitivity_ol_controls}}
        \caption[Open-loop control parameter sensitivity]{Sensitivity of the open-loop control strategy to parameterisation. Each row corresponds to a single random parameter perturbation, with the rows ordered by the objective value under open-loop control. \textbf{(a)} shows the change in objective from the baseline parameterisation, with negative values signalling worse epidemics. \textbf{(b)}, \textbf{(c)} and \textbf{(d)} show the allocation of control resources over time to thinning, roguing and protecting control methods respectively. All strategies carry out thinning early in the epidemic, with roguing throughout and protection only after thinning is carried out.\label{fig:ch6:ol_sensitivity}}
    \end{center}
\end{figure}

Figure~\ref{fig:ch6:mpc_sensitivity} shows similar results using the MPC framework. Here we can clearly see the additional thinning carried out over the course of the epidemic, at least for many of the parameter sets. The strategies remain similar in structure, with most of the thinning carried out early in the epidemic, roguing throughout, and protection after the initial thinning regime. The difference in objective is here calculated relative to the baseline MPC strategy. Once again, there is little systematic structure when ordered by objective.

\begin{figure}[t]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Sensitivity_mpc_controls}}
        \caption[MPC strategy parameter sensitivity]{Sensitivity of the MPC strategy to parameterisation. As in Figure~\ref{fig:ch6:ol_sensitivity}, the control allocations are shown ordered by objective value, but here using the MPC framework updating every 20 years. Additional thinning is seen compared to open-loop, but no strong systematic pattern is seen.\label{fig:ch6:mpc_sensitivity}}
    \end{center}
\end{figure}

\subsubsection{Parameter uncertainty}

In reality, precise values of infection rates are never known. These parameters are often fitted to limited data with Bayesian techniques, giving a probability distribution of values \citep[e.g.][]{kleczkowski_parameter_2007, parry_bayesian_2014, thompson_control_2018, cunniffe_cost-effective_2014}. In this section we test how the open-loop and MPC frameworks handle this type of uncertainty in the system dynamics. Uncertainty is introduced by sampling values from a distribution of infection rates for each species in the simulation model. What benefit does feedback in the MPC framework have when the approximate model cannot accurately capture the dynamics of each individual realisation?

The parameter distribution is chosen to be normal (truncated so that infection parameters remain positive), and the standard deviation for each infection rate is set to \SI{40}{\percent} of the parameter value. The approximate model is re-fitted to an ensemble of 200 simulations (Figure~\ref{fig:ch6:param_uncert}(a) and (b)), with each simulation using a set of infection rates drawn from the distribution. The approximate model uses the same roguing rate scaling factor as previously found (Section~\ref{sec:ch6:control_scaling}, page~\pageref{sec:ch6:control_scaling}).

\begin{figure}[t]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Param_uncert}}
        \caption[Effect of parameter uncertainty on control performance]{The effect of parameter uncertainty on control performance. The approximate model is fitted to an ensemble of simulation runs without control, with infection rate parameters drawn from a truncated normal distribution. \textbf{(a)} shows the ensemble and fitted infected tanoak dynamics, and \textbf{(b)} the ensemble and fitted infected bay dynamics. \textbf{(c)} shows the distribution of objective values using open-loop and MPC across 200 draws of simulation parameters. The absolute improvement of the MPC strategy over open-loop is shown in \textbf{(d)}, as a function of the open-loop objective. The MPC framework performs well in the worst-case scenarios, improving control to the largest extent when open-loop performs badly. Four individual cases have been highlighted in panels \textbf{(c)} and \textbf{(d)}. Further details for each of these, highlighting how rates of spread drive the differences in performance, are shown in Appendix~\ref{app:mixed_stand_1}, p.~\pageref{app:mixed_stand_1}.\label{fig:ch6:param_uncert}}
    \end{center}
\end{figure}

To test control on these parameter distributions, the fitted approximate model is used to run the open-loop and MPC frameworks for a single draw of infection rates from the distribution. This is repeated for 200 draws from each distribution. The distribution of the resulting objective values under the open-loop and MPC frameworks shows that MPC improves the worst-case scenarios, i.e.\ the MPC updates are most beneficial when the disease is hardest to manage (Figure~\ref{fig:ch6:param_uncert}(c)). The open-loop framework gives a distribution of objectives with a worse minimum value than MPC. The continued surveillance of MPC generally improves control, but the greatest improvements are seen when the epidemic is difficult to control, making the open-loop framework ineffective (Figure~\ref{fig:ch6:param_uncert}(d)). When objective values are high, and so the epidemic is easy to control, there is little difference between open-loop and MPC. MPC is therefore useful for limiting the worst case scenarios under parameter uncertainty, highlighting the importance of continued surveillance when disease progression cannot be predicted accurately.

In the open-loop case there is a large peak at very low objective values. In these cases the control strategy fails to manage the epidemic and almost all tanoak is lost. The peak is less prominent in the MPC case since the updates allow control to respond to unexpected changes. This can result in more intermediate performance because outbreaks can be caught at an earlier stage.

\FloatBarrier
\subsubsection{Observational uncertainty}

The re-optimisation of control in the MPC framework requires accurate information about the current state of the forest at each update step, i.e.\ every \SI{20}{\years} by default. However, full forest stand surveys are expensive. We test here whether the intensity of these update surveys can be reduced whilst maintaining effective control, and whether open-loop strategies can ever outperform MPC with low quality surveillance.

At each MPC update time a proportion of the forest stand is sampled. The measured state is then used as the initial condition in the approximate model for re-optimisation of the control. For the sampling, each cell in the simulation model is split into 500 \SI{1}{\meter\squared} discrete units. Surveillance at update times is then carried out by observing a fixed number of units in each cell across the landscape, without replacement. The infection status of tanoak and bay hosts is determined randomly, with probabilities matching the proportion of that host that is infected.

\begin{figure}[t]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Obs_uncert}}
        \caption[Performance of MPC under observational uncertainty]{\textbf{(a)} shows the objective value when there is observational uncertainty at MPC updates. The uncertainty is modelled as random sampling of a proportion of the forest stand area. Shaded areas show the 5\textsuperscript{th} to 95\textsuperscript{th} percentiles of the objective values. In \textbf{(b)} the costs associated with control and update surveys are shown. Surveillance costs increase as surveillance becomes more intensive, but the benefits of improved control (negative costs) also increase. \textbf{(c)} shows the corresponding overall cost under observational uncertainty, including costs for MPC update surveys. Reduced costs of less intensive surveying must be balanced with less effective control when there is uncertainty about the pathogen extent.\label{fig:ch6:obs_uncert}}
    \end{center}
\end{figure}

As the proportion of area sampled decreases, the uncertainty in the outcome of MPC increases (Figure~\ref{fig:ch6:obs_uncert}(a)). The median performance of MPC also decreases. This is because as less of the forest is sampled, there is a higher chance that infected hosts will be missed during surveillance, and so the rate of disease spread will be underestimated. We introduce a fixed cost per unit area sampled for the update step surveys, and negate the objective function value as a measure of management benefits. The overall cost is then given by:
\begin{equation}
    \textrm{cost} = k_1Np - k_2J
\end{equation}
where $k_1$ and $k_2$ are arbitrary constants, $N$ is the number of surveys carried out surveying a proportion $p$ of the forest, and $J$ is the management objective function. The choice of constants $k_1$ and $k_2$ is arbitrary, and must be carried out by a forest manager since it balances management and surveillance costs with diversity and tanoak retention benefits. We here choose, as an example, $k_1= 10$ and $k_2 = 45$. As the proportion of the stand that is surveyed increases, so too do the surveillance costs (Figure~\ref{fig:ch6:obs_uncert}(b)). The disease costs however, reduce with more tanoak retained as a result of more informed and effective control strategies. 

Balancing these two costs results in an optimal level of surveillance effort (Figure~\ref{fig:ch6:obs_uncert}(c)). The precise location of this optimum depends on the balance between tanoak retention, biodiversity conservation, and surveillance costs: a decision that must be made in the context of local forest management goals. It is clear though, that some level of continued surveillance and re-optimisation through the MPC framework is necessary for effective control. It can also be seen that to minimise the 95\textsuperscript{th} percentile a higher intensity of surveillance is required. This would represent the control policy of a highly risk-averse manager.

\FloatBarrier
\newpage
\subsection{Refined optimal strategy\label{sec:ch6:refined_strat}}

When the approximate model was parameterised, the roguing rate was scaled so that the model matched the simulation as accurately as possible under control. So as not to bias results no assumptions were made about the form of the control, and the scaling was carried out using a constant roguing rate. Now that the optimal strategy has been found, we are in a position to re-consider this scaling to parameterise the best possible fitting approximate model. Whilst in most real applications of open-loop and MPC this would not be possible due to uncertainties in the simulation model, lack of data or stochasticity for example, here we can test the performance of a refined, optimal approximate model.

The roguing rate in the approximate model was rescaled using the open-loop control strategy found in Section~\ref{sec:ch6:open_loop}. This ensures the approximate model fits best under the type of control that will be optimal. As before in Figure~\ref{fig:ch6:control_scaling}, the roguing rate was scaled to best match the final number of tanoaks in the simulation model. As shown in Figure~\ref{fig:ch6:Global_optimal}(a) this gives a much lower scaling factor than was previously obtained.

When this new approximate model is optimised, the control strategy found is very similar to the previous open-loop strategy (Figure~\ref{fig:ch6:Global_optimal}(b)), but the approximate model fits the simulation much better (Figure~\ref{fig:ch6:Global_optimal}(d)). This results in control that is more effective than either the open-loop or MPC strategies found previously. Since the approximate model fits better, there is little benefit to the repeated updates of the MPC framework, and so MPC using the newly parameterised model is only a marginal improvement over open-loop (Figure~\ref{fig:ch6:Global_optimal}(c)).

\begin{figure}[H]
    \begin{center}
        \makebox[\textwidth][c]{\includegraphics{Graphics/ch6/Global_optimal}}
        \caption[Refined optimal strategy under rescaled roguing rate]{Rescaling the roguing rate in the approximate model to find a refined optimal strategy. \textbf{(a)} shows the optimisation of the new roguing rate scaling factor, chosen to minimise the deviation in final tanoak numbers between approximation and simulation. The optimal scaling factor under the open-loop control strategy was found to be 0.168 (3~s.f.). \textbf{(b)} shows the open-loop control strategy found using this newly parameterised model, with the state dynamics shown in \textbf{(c)}. The approximate model matches the simulation dynamics very closely. This allows improved disease management, as shown in \textbf{(d)}. The newly scaled frameworks outperform the previous strategies.\label{fig:ch6:Global_optimal}}
    \end{center}
\end{figure}

\section{Discussion\label{sec:ch6:discussion}}

In this chapter we have seen how the frameworks developed in Chapter~\ref{ch:complex_models} can be applied to a real-world scenario: control of sudden oak death in forest stands. As we found in Chapter~\ref{ch:complex_models} in a more theoretical setting, the feedback in the MPC framework improves management, and leads to more robust control. In this section we will discuss how these results relate to practical management of SOD.

\subsection{Practical application of management strategies\label{sec:ch6:discussion_practical}}

Current advice for management of SOD centres around removal of the spreader species, bay laurel, as well as infected tanoak and bay, and susceptible tanoak close to known infections \citep{comtf_sudden_2014}. Application of protective chemical treatments is also recommended for high value trees that are close to infections and known not to be currently infected \citep{lee_protecting_2010}. The strategies found in this chapter using OCT are broadly similar in nature to these recommendations. We have found that thinning of bay laurel is very important to the success of disease management, echoing results of previous modelling studies \citep{cobb_resiliency_2017,ndeffo_mbah_optimization_2010}. Management advice from the US Forest Service \citep{swiecki_reference_2013} also suggests removal of bay trees, and even complete area-wide removal of bay in some cases. Area-wide removal is only recommended when bay is a minor forest component or consists of only small trees, and only when this removal is consistent with other management goals. We found that roguing of tanoak is more important than roguing of infected bay, as thinning would reduce the bay population density sufficiently for disease control. Our results show that with continued surveillance and careful optimisation of controls, disease can be successfully managed. This can be done whilst maintaining a bay population which may be ecologically important. However, when biodiversity benefits are less important, control is always easier and more effective with additional removal of bay laurel.

Application of chemical protectants is only recommended in practice for individual high value trees close to known infections \citep{comtf_sudden_2014}. The strategies we have found deploy significant protection resources though. In fact, the protectant application only has a very minor effect on the performance of the strategies (see Appendix~\ref{app:mixed_stand_2}, p.~\pageref{app:mixed_stand_2}), and is unlikely to be cost-effective. Our formulation of the budget constraint implements a maximum expenditure where a fixed amount of money is put aside for SOD control, rather than minimising total costs. This captures governmental allocation of money for SOD control. In our model, when the optimal levels of roguing and thinning do not use the entire budget, the surplus can be allocated to protectant application. In practice though, control methods will also be individually assessed for cost-effectiveness, and given the limited effect of the protectant strategies it seems unlikely that they would be used.

A possible limitation of the strategies we have found is their complexity. Control inputs were held constant over \SI{5}{\year} stages so that resources do not have to be continually moved, but the strategies are nevertheless still complex in their time dependence and relative allocations to multiple control methods. However, the findings of our results could still be useful. Building up an intuition about what drives the optimal strategy could lead to more practical advice. For example, in the open-loop and MPC strategies thinning of bay is carried out early before switching to thinning of redwood. These species are reduced to a threshold density that OCT has identified as sufficiently low to suppress pathogen spread, and promote tanoak restoration. This type of insight about optimal densities of different species in mixed stands over the course of an epidemic could provide actionable advice for foresters.

\subsection{Choosing management goals}

The complexity of the optimal strategies found comes in part from balancing multiple costs and benefits: tanoak retention, biodiversity conservation, control expenditure and surveillance costs. The valuation of the cultural and ecological benefits against more direct economic costs is a difficult decision that must be taken by forest managers. These decisions must be made in the context of the local area, as well as other forest management goals such as fire reduction and timber production \citep{cobb_resiliency_2017}. Decisions about management goals however, either locally or through larger scale regulation, can lead to conflicts that may impact the effectiveness of control \citep{alexander_lessons_2010}. The value of tanoak retention must be balanced carefully with the wider impacts of the control on the forest.

As well as valuation, the formulation of the different cost functions is important. Here, we used a metric for biodiversity conservation which was integrated over time and so ensures biodiversity is conserved at all times. This captures the importance of continued biodiversity for wildlife habitats, but also avoids introducing edge effects, for example thinning very late in the epidemic to meet a biodiversity target. Both the biodiversity and tanoak objectives introduce a dependence on the chosen time horizon of \SI{100}{\years}, but the tanoak objective only depends on the amount of tanoak at the final time. This final time dependence is appropriate for a restoration type management goal, such as ensuring a resource is available in the future. There is still however, a flexibility in the form of the objective function chosen, and the precise choice of objective does impact disease control \citep{probert_decision_2016}. However, extending the final time out to \SI{160}{\years} results in functional extinction of tanoak in the open-loop case (i.e.\ no significant ecosystem function), but retention of overstorey tanoak when using MPC (see Appendix~\ref{app:mixed_stand_3}, p.~\pageref{app:mixed_stand_3}).

It is also important to consider the effect of the control strategies on the forest. Whilst the effects of the disease can be devastating---particularly to tanoak populations---the optimal control strategies remove large numbers of trees, including trees that would not have died from the disease. This could be more damaging to the forest ecosystem than the disease effects would have been. \citet{alexander_lessons_2010} write that one land manager who supported removal of tanoak and bay laurel feared an outcome of `destroy the village to save it'. A difficult problem for forest managers is when control should be abandoned, as has been done in the `Generally Infested Area' in Oregon \citep{hansen_efficacy_2019}.

\subsection{Continued surveillance and re-optimisation}

We have shown that by repeated surveillance and re-optimisation of control, the damaging effects of the worst-case scenarios of pathogen spread can be limited. As found by \citet{cobb_resiliency_2017}, disease control is only effective when there is long-term commitment to management projects. Effort put into this long-term surveillance has to be cost-effective though. With imperfect surveillance introducing observational uncertainty, an optimal balance between survey costs and epidemic control was found. Other modelling studies of SOD have also found that resource constraints lead to a trade off between detection and control \citep{ndeffo_mbah_balancing_2010, cunniffe_modelling_2016}. However, our analysis does not incorporate the risk of disease re-emergence. Our model is deterministic, so also does not capture stochastic re-introductions. Furthermore, the state of the epidemic in the wider region will impact the risk of re-emergence through potentially increasing inoculum pressure, for example from the advancing wavefront of other epidemics in the local vicinity. Forest managers must take into account these other factors in determining optimal levels of surveillance as well as in designing controls, but regardless we have shown that vigilance to disease progression is important. Alongside this vigilance must be a willingness to adapt control measures, re-optimising control to suit the current state of the epidemic and changing local management goals.

\subsection{Robust control}

We showed in Section~\ref{sec:ch6:robust} that the general form of the optimal control strategy is robust to changes in parameterisation. In all cases a period of thinning is carried out before protection starts, and roguing is carried out throughout the epidemic. The relative performance though, both in parameter sensitivity and parameter uncertainty studies, can vary significantly. In Figure~\ref{fig:ch6:param_uncert} we saw that under high levels of parameter uncertainty, some epidemics become much easier to control, and others much harder. In the open-loop case this leads to a split in the distribution of objective values into those where initial control manages the disease, and those where re-emergence leads to significant loss of tanoak. This shows how a poorly designed strategy can lead to significant disease impacts.

In testing robustness we showed that the MPC framework is able to mitigate the effects of the most damaging epidemics, improving management performance in the worst-case scenarios. \citet{maccleery_reinventing_2015} states that a major barrier to US Forest Service management is the opposition to adaptive management, in which ongoing monitoring is used to update management advice. This is seen as too `experimental' and increases short-term risk, but imposing fixed interventions means strategies cannot be adapted based on what is seen `on the ground'. Here we have shown a clear benefit to the ongoing surveillance and re-optimisation of control, with MPC as a possible formal framework for adapting strategies. Even the simpler open-loop framework still significantly slowed tanoak decline though, and slowing pathogen spread is still a useful goal allowing time to prepare for ecosystem impacts \citep{cobb_biodiversity_2013}.


\section{Conclusions\label{sec:ch6:conclusions}}

In this chapter we have optimised strategies for slowing, or even halting, pathogen-induced decline of tanoak in mixed species forest stands. OCT was used to find optimal time-dependent deployment of thinning, roguing, and protectant control resources. The strategies found are broadly consistent with current expert advice: focussing on thinning of bay laurel and roguing of infected trees. However, the strategies we found show significant time dependence. Continued surveillance and re-optimisation of the control strategy by using the MPC framework improves control performance. MPC leads to robust strategies that can effectively respond to unanticipated disease dynamics and system uncertainties, and so manage SOD to protect valuable tanoak trees whilst also conserving biodiversity.
